{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacepy import pycdf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "#  Get the directory where the Jupyter Notebook is located\n",
    "notebook_dir = \"/home/lyra/Documents/Solar Flares/Data_new/comp\"\n",
    "\n",
    " # Define the path to the home folder\n",
    "home_path = \"/home/lyra\"\n",
    "\n",
    "# # Change the current working directory to the home folder\n",
    "os.chdir(home_path)\n",
    "\n",
    "# # Iterate through all files with the .nc.gz extension in the home directory\n",
    "for file in glob.glob(\"*.nc.gz\"):\n",
    "     # Decompress the file and save it in the \"Data\" folder\n",
    "    with gzip.open(file, 'rb') as f_in:\n",
    "        decompressed_filename = os.path.join(notebook_dir, file[:-3])\n",
    "        with open(decompressed_filename, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Decompression and saving to 'Data' folder completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Define the paths to the 'Data' and 'CSV' folders\n",
    "data_folder = \"/home/lyra/Documents/Solar Flares/Data_new/comp\"\n",
    "csv_folder = \"/home/lyra/Documents/Solar Flares/Data_new/comp_csv\"\n",
    "\n",
    "# Create the 'CSV' folder if it doesn't exist\n",
    "if not os.path.exists(csv_folder):\n",
    "    os.makedirs(csv_folder)\n",
    "\n",
    "# Check for files with .nc extension in the 'Data' folder\n",
    "nc_files = glob.glob(os.path.join(data_folder, \"*.nc\"))\n",
    "\n",
    "if not nc_files:\n",
    "    print(\"No .nc files found in the 'Data' folder.\")\n",
    "else:\n",
    "    # Loop through the .nc files and convert them to CSV, handling errors\n",
    "    for nc_file in nc_files:\n",
    "        file_name = os.path.basename(nc_file)  # Get the file name without path\n",
    "        csv_file = os.path.join(csv_folder, os.path.splitext(file_name)[0] + \".csv\")\n",
    "\n",
    "        try:\n",
    "            # Open the .nc file using netCDF4\n",
    "            nc = Dataset(nc_file, 'r')\n",
    "\n",
    "            # Create a dictionary to store variable data\n",
    "            variable_data = {}\n",
    "\n",
    "            # Extract all variables and their data\n",
    "            for var_name, var in nc.variables.items():\n",
    "                variable_data[var_name] = var[:]\n",
    "\n",
    "            # Create a Pandas DataFrame from the variable data\n",
    "            df = pd.DataFrame(variable_data)\n",
    "\n",
    "            # Save the DataFrame as a CSV file\n",
    "            df.to_csv(csv_file, index=False)\n",
    "\n",
    "            print(f\"Converted {file_name} to {os.path.basename(csv_file)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {file_name}: {str(e)}\")\n",
    "\n",
    "# Now, create a Pandas DataFrame from the CSV files in the 'CSV' folder\n",
    "csv_files = glob.glob(os.path.join(csv_folder, \"*.csv\"))\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found in the 'CSV' folder.\")\n",
    "else:\n",
    "    # Concatenate the CSV files into a Pandas DataFrame\n",
    "    df = pd.concat(map(pd.read_csv, csv_files))\n",
    "    print(\"Concatenation of CSV files completed.\")\n",
    "\n",
    "# Now, 'df' contains the combined data from all CSV files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "def generate_data_profiling_report(csv_folder):\n",
    "    # Initialize an empty list to store profiling reports\n",
    "    profiling_reports = []\n",
    "\n",
    "    # Create the 'Data Profiles' folder if it doesn't exist\n",
    "    output_folder = '/home/lyra/Documents/Solar Flares/Data Profiles'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # List all files in the CSV folder\n",
    "    csv_files = [f for f in os.listdir(csv_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "    # Loop through each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(csv_folder, csv_file)\n",
    "\n",
    "            # Read the CSV file into a pandas DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Generate a data profiling report for the DataFrame\n",
    "            report = ProfileReport(df, title=f'Data Profiling Report - {csv_file}')\n",
    "\n",
    "            # Define the report file path\n",
    "            report_path = os.path.join(output_folder, f'report_{csv_file}.html')\n",
    "\n",
    "            # Append the report to the list\n",
    "            profiling_reports.append(report)\n",
    "\n",
    "            # Save the report as an HTML file in the 'Data Profiles' folder\n",
    "            report.to_file(report_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_file}: {str(e)}\")\n",
    "\n",
    "    return profiling_reports\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Folder containing CSV files to profile\n",
    "    csv_folder = \"/home/lyra/Documents/Solar Flares/CSV\"  # Replace with the actual path to your CSV folder\n",
    "\n",
    "    # Generate data profiling reports\n",
    "    reports = generate_data_profiling_report(csv_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert JSON to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "# URL of the JSON data\n",
    "json_url = 'https://services.swpc.noaa.gov/products/noaa-planetary-k-index-forecast.json'\n",
    "\n",
    "try:\n",
    "    # Retrieve JSON data from the URL\n",
    "    response = requests.get(json_url)\n",
    "    response.raise_for_status()  # Check for any HTTP errors\n",
    "\n",
    "    # Parse the JSON data\n",
    "    data = response.json()\n",
    "\n",
    "    if not data:\n",
    "        print(f'JSON data from {json_url} is empty.')\n",
    "    else:\n",
    "        # Specify the CSV file name\n",
    "        csv_file = '/home/lyra/Documents/Solar Flares/json_data.csv'\n",
    "\n",
    "        # Write JSON data to CSV\n",
    "        with open(csv_file, 'w', newline='') as csv_file:\n",
    "            # Create a CSV writer\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "\n",
    "            # Write the data rows\n",
    "            csv_writer.writerows(data)\n",
    "\n",
    "        print(f'JSON data from {json_url} has been converted to CSV and saved as {csv_file}.')\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f'Error fetching data from the URL: {str(e)}')\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f'Error decoding JSON data: {str(e)}')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One file y-data profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2964/401332772.py:2: DeprecationWarning: `import pandas_profiling` is going to be deprecated by April 1st. Please use `import ydata_profiling` instead.\n",
      "  from pandas_profiling import ProfileReport\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4229c1c441244388d9bd6b025de3d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d85e6489e0451f944f34dfcd2b45f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701721281073439f9db0a9edb32eab33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5eececb7d7433c9932fe81c47355cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data profiling report saved to /home/lyra/Documents/Solar Flares/Data_new/new.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "csv_file = \"/home/lyra/Documents/Solar Flares/Merged_Dataset.csv\"  # Replace with the path to your CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Generate a profile report\n",
    "profile = ProfileReport(df, title=\"Data Profiling Report\", explorative=True)\n",
    "\n",
    "# Save the report to an HTML file\n",
    "report_file = \"/home/lyra/Documents/Solar Flares/Data_new/new.html\"  # Replace with the desired output file path\n",
    "profile.to_file(report_file)\n",
    "\n",
    "# Display the report in Jupyter Notebook (optional)\n",
    "# profile.to_notebook_iframe()\n",
    "\n",
    "print(f\"Data profiling report saved to {report_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert given timestamp to human - readable form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to convert Unix timestamp to datetime\n",
    "def unix_timestamp_to_datetime(timestamp_ms):\n",
    "    # Divide by 1000 to convert from milliseconds to seconds\n",
    "    timestamp_seconds = timestamp_ms / 1000\n",
    "    # Create a datetime object\n",
    "    dt = datetime.utcfromtimestamp(timestamp_seconds)\n",
    "    return dt\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('/home/lyra/Documents/Solar Flares/CSV/Combined/combined_pop.csv')\n",
    "\n",
    "# Convert the 'time' column to numeric values (assuming it contains strings)\n",
    "df['time'] = pd.to_numeric(df['time'], errors='coerce')\n",
    "\n",
    "# Remove rows with NaN values in the 'time' column\n",
    "df = df.dropna(subset=['time'])\n",
    "\n",
    "# Apply the unix_timestamp_to_datetime function to the 'time' column\n",
    "df['time'] = df['time'].apply(unix_timestamp_to_datetime)\n",
    "\n",
    "# Store the DataFrame with the converted 'time' column back to a CSV file\n",
    "output_csv_file = '/home/lyra/Documents/Solar Flares/CSV/Combined/new_pop.csv'\n",
    "df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "print(f'The DataFrame has been saved to \"{output_csv_file}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All csv time format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to convert Unix timestamp to datetime\n",
    "def unix_timestamp_to_datetime(timestamp_ms):\n",
    "    # Divide by 1000 to convert from milliseconds to seconds\n",
    "    timestamp_seconds = timestamp_ms / 1000\n",
    "    # Create a datetime object\n",
    "    dt = datetime.utcfromtimestamp(timestamp_seconds)\n",
    "    return dt\n",
    "\n",
    "# Specify the input folder containing CSV files\n",
    "input_folder = '/home/lyra/Documents/Solar Flares/Data_new/comp_csv'\n",
    "\n",
    "# Specify the output folder where converted CSV files will be saved\n",
    "output_folder = '/home/lyra/Documents/Solar Flares/Data_new/conv_csv'\n",
    "\n",
    "# Iterate through each CSV file in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Construct the full paths for input and output files\n",
    "        input_csv_file = os.path.join(input_folder, filename)\n",
    "        output_csv_file = os.path.join(output_folder, filename)\n",
    "\n",
    "        # Load the CSV file into a DataFrame\n",
    "        df = pd.read_csv(input_csv_file)\n",
    "\n",
    "        # Convert the 'time' column to numeric values (assuming it contains strings)\n",
    "        df['time'] = pd.to_numeric(df['time'], errors='coerce')\n",
    "\n",
    "        # Remove rows with NaN values in the 'time' column\n",
    "        df = df.dropna(subset=['time'])\n",
    "\n",
    "        # Apply the unix_timestamp_to_datetime function to the 'time' column\n",
    "        df['time'] = df['time'].apply(unix_timestamp_to_datetime)\n",
    "\n",
    "        # Store the DataFrame with the converted 'time' column back to a CSV file\n",
    "        df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "        print(f'The DataFrame from \"{input_csv_file}\" has been saved to \"{output_csv_file}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COmbining based on 'time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "df_f = pd.read_csv('/home/lyra/Documents/Solar Flares/Data_new/conv_csv/merged_f1m.csv')\n",
    "df_m = pd.read_csv('/home/lyra/Documents/Solar Flares/Data_new/conv_csv/merged_m1m.csv')\n",
    "\n",
    "# Assuming 'time' is the name of the common timestamp column\n",
    "# Convert the 'time' column to datetime objects\n",
    "df_f['time'] = pd.to_datetime(df_f['time'])\n",
    "df_m['time'] = pd.to_datetime(df_m['time'])\n",
    "\n",
    "# Extract date and hour components from the 'time' column\n",
    "df_f['date'] = df_f['time'].dt.date\n",
    "df_f['hour'] = df_f['time'].dt.hour\n",
    "\n",
    "df_m['date'] = df_m['time'].dt.date\n",
    "df_m['hour'] = df_m['time'].dt.hour\n",
    "\n",
    "# Drop the 'time' columns from the original DataFrames\n",
    "df_f = df_f.drop(columns=['time'])\n",
    "df_m = df_m.drop(columns=['time'])\n",
    "\n",
    "# Merge the DataFrames based on date and hour\n",
    "merged_df = pd.merge(df_f, df_m, on=['date', 'hour'], how='inner')\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('/home/lyra/Documents/Solar Flares/Data_new/conv_csv/Final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Kp index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "df_json = pd.read_csv('/home/lyra/Documents/Solar Flares/json_data.csv')\n",
    "df_merged = pd.read_csv('/home/lyra/Documents/Solar Flares/merged.csv')\n",
    "\n",
    "# Assuming 'time tag' and 'time' are the timestamp columns\n",
    "# Convert the timestamp columns to datetime objects\n",
    "df_json['time_tag'] = pd.to_datetime(df_json['time_tag'])\n",
    "df_merged['time'] = pd.to_datetime(df_merged['time'])\n",
    "\n",
    "# Extract the date and hour components from 'time_tag' and 'time'\n",
    "df_json['date_tag'] = df_json['time_tag'].dt.strftime('%Y-%m-%d %H')\n",
    "df_merged['date'] = df_merged['time'].dt.strftime('%Y-%m-%d %H')\n",
    "\n",
    "# Merge the DataFrames based on the 'date_tag' and 'date' columns\n",
    "merged_df = pd.merge(df_json, df_merged, left_on='date_tag', right_on='date', how='inner')\n",
    "\n",
    "# Drop the duplicate timestamp columns ('time', 'date', 'date_tag')\n",
    "merged_df = merged_df.drop(columns=['time', 'date', 'date_tag'])\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "# Set the 'time_tag' column as the index and sort by it\n",
    "merged_df.set_index('time_tag', inplace=True)\n",
    "merged_df.sort_index(inplace=True)\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "merged_df.to_csv('/home/lyra/Documents/Solar Flares/C.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10080, 37)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"/home/lyra/Documents/Solar Flares/Data_new/conv_csv/merged_both.csv\")\n",
    "# print(number of rows and columns)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 3)\n"
     ]
    }
   ],
   "source": [
    "kp = pd.read_csv(\"/home/lyra/Documents/Solar Flares/Data_new/kpc.csv\")\n",
    "print(kp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging only known kp to merged_both.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files into Pandas DataFrames\n",
    "merged_both_df = pd.read_csv('/home/lyra/Documents/Solar Flares/Data_new/conv_csv/merged_both.csv')\n",
    "kpc_df = pd.read_csv('/home/lyra/Documents/Solar Flares/Data_new/kpc.csv')\n",
    "\n",
    "# Merge the DataFrames based on the 'time' column using a left join\n",
    "merged_df = pd.merge(merged_both_df, kpc_df, on='time', how='left')\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('/home/lyra/Documents/Solar Flares/Data_new/test_final.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
